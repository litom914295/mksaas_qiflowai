# Phase 7 å®æ–½è®¡åˆ’ - RAG çŸ¥è¯†åº“é›†æˆ

**ç›®æ ‡**: é€šè¿‡ RAG (Retrieval-Augmented Generation) æŠ€æœ¯å¢å¼º AI å›ç­”è´¨é‡  
**é¢„è®¡è€—æ—¶**: 12 å°æ—¶  
**ä¼˜å…ˆçº§**: ä¸­é«˜

---

## ğŸ¯ æ”¹é€ ç›®æ ‡

### 1. çŸ¥è¯†åº“ä½“ç³»
- ğŸ“š å…«å­—ç»å…¸æ–‡çŒ® (æ»´å¤©é«“ã€ä¸‰å‘½é€šä¼šã€æ¸Šæµ·å­å¹³)
- ğŸ  é£æ°´ç»å…¸æ–‡çŒ® (å®…ç»ã€é˜³å®…ä¸‰è¦ã€ç„ç©ºé£æ˜Ÿ)
- ğŸ”® ç°ä»£åº”ç”¨æ¡ˆä¾‹
- ğŸ“– FAQ å¸¸è§é—®é¢˜

### 2. æŠ€æœ¯æ¶æ„
- ğŸ” å‘é‡åŒ–å¼•æ“ (OpenAI Embeddings)
- ğŸ’¾ å‘é‡æ•°æ®åº“ (Supabase pgvector)
- ğŸ” è¯­ä¹‰æ£€ç´¢ (Similarity Search)
- ğŸ¤– RAG å¢å¼ºç”Ÿæˆ
- ğŸ“ çŸ¥è¯†å¼•ç”¨å±•ç¤º

### 3. æ€§èƒ½ç›®æ ‡
- æ£€ç´¢å»¶è¿Ÿ: < 200ms
- å¬å›ç‡: > 80%
- å‡†ç¡®ç‡: > 90%
- å•æ¬¡æˆæœ¬: < $0.02

---

## ğŸ“‹ å®æ–½æ­¥éª¤

### Step 1: æ•°æ®åº“ Schema (1 å°æ—¶)

#### 1.1 åˆ›å»ºå‘é‡è¡¨
**æ–‡ä»¶**: `drizzle/0004_phase7_knowledge_base.sql`

```sql
-- å¯ç”¨ pgvector æ‰©å±•
CREATE EXTENSION IF NOT EXISTS vector;

-- çŸ¥è¯†åº“æ–‡æ¡£è¡¨
CREATE TABLE knowledge_documents (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  
  -- æ–‡æ¡£ä¿¡æ¯
  title TEXT NOT NULL,
  category TEXT NOT NULL, -- 'bazi' | 'fengshui' | 'faq' | 'case'
  source TEXT NOT NULL, -- æ¥æºä¹¦ç±/ç½‘ç«™
  content TEXT NOT NULL,
  
  -- å…ƒæ•°æ®
  metadata JSONB DEFAULT '{}',
  
  -- å‘é‡ (1536 ç»´ - OpenAI text-embedding-3-small)
  embedding vector(1536),
  
  -- ç»Ÿè®¡
  chunk_index INTEGER DEFAULT 0, -- åˆ†å—ç´¢å¼•
  parent_doc_id UUID REFERENCES knowledge_documents(id), -- çˆ¶æ–‡æ¡£ ID (ç”¨äºåˆ†å—)
  view_count INTEGER DEFAULT 0,
  reference_count INTEGER DEFAULT 0,
  
  -- æ—¶é—´æˆ³
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() NOT NULL,
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() NOT NULL
);

-- ç´¢å¼•
CREATE INDEX knowledge_documents_category_idx ON knowledge_documents(category);
CREATE INDEX knowledge_documents_source_idx ON knowledge_documents(source);
CREATE INDEX knowledge_documents_parent_doc_idx ON knowledge_documents(parent_doc_id);

-- å‘é‡ç›¸ä¼¼åº¦æœç´¢ç´¢å¼• (HNSW)
CREATE INDEX knowledge_documents_embedding_idx ON knowledge_documents 
USING hnsw (embedding vector_cosine_ops);

-- RAG æ£€ç´¢å†å²è¡¨
CREATE TABLE rag_retrieval_logs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  
  -- æŸ¥è¯¢ä¿¡æ¯
  user_id TEXT NOT NULL,
  session_id UUID,
  query TEXT NOT NULL,
  query_embedding vector(1536),
  
  -- æ£€ç´¢ç»“æœ
  retrieved_doc_ids UUID[] NOT NULL,
  top_k INTEGER NOT NULL DEFAULT 3,
  similarity_scores FLOAT[] NOT NULL,
  
  -- ç”Ÿæˆç»“æœ
  generated_response TEXT,
  model TEXT NOT NULL,
  
  -- å…ƒæ•°æ®
  metadata JSONB DEFAULT '{}',
  
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() NOT NULL
);

-- ç´¢å¼•
CREATE INDEX rag_retrieval_logs_user_idx ON rag_retrieval_logs(user_id);
CREATE INDEX rag_retrieval_logs_session_idx ON rag_retrieval_logs(session_id);
CREATE INDEX rag_retrieval_logs_created_at_idx ON rag_retrieval_logs(created_at);
```

#### 1.2 Drizzle Schema å®šä¹‰
**æ–‡ä»¶**: `src/db/schema-knowledge.ts`

```typescript
import { pgTable, uuid, text, timestamp, jsonb, integer, index, vector } from "drizzle-orm/pg-core";
import { user } from "./schema";

// çŸ¥è¯†åº“æ–‡æ¡£è¡¨
export const knowledgeDocuments = pgTable(
  "knowledge_documents",
  {
    id: uuid("id").primaryKey().defaultRandom(),
    
    title: text("title").notNull(),
    category: text("category").notNull(), // 'bazi' | 'fengshui' | 'faq' | 'case'
    source: text("source").notNull(),
    content: text("content").notNull(),
    
    metadata: jsonb("metadata").$type<Record<string, unknown>>().default({}),
    
    // pgvector: vector(1536) for OpenAI embeddings
    embedding: vector("embedding", { dimensions: 1536 }),
    
    chunkIndex: integer("chunk_index").default(0),
    parentDocId: uuid("parent_doc_id"),
    viewCount: integer("view_count").default(0),
    referenceCount: integer("reference_count").default(0),
    
    createdAt: timestamp("created_at").notNull().defaultNow(),
    updatedAt: timestamp("updated_at").notNull().defaultNow(),
  },
  (table) => ({
    categoryIdx: index("knowledge_documents_category_idx").on(table.category),
    sourceIdx: index("knowledge_documents_source_idx").on(table.source),
    parentDocIdx: index("knowledge_documents_parent_doc_idx").on(table.parentDocId),
    // HNSW index created in SQL migration
  })
);

// RAG æ£€ç´¢å†å²è¡¨
export const ragRetrievalLogs = pgTable(
  "rag_retrieval_logs",
  {
    id: uuid("id").primaryKey().defaultRandom(),
    
    userId: text("user_id").notNull(),
    sessionId: uuid("session_id"),
    query: text("query").notNull(),
    queryEmbedding: vector("query_embedding", { dimensions: 1536 }),
    
    retrievedDocIds: jsonb("retrieved_doc_ids").$type<string[]>().notNull(),
    topK: integer("top_k").notNull().default(3),
    similarityScores: jsonb("similarity_scores").$type<number[]>().notNull(),
    
    generatedResponse: text("generated_response"),
    model: text("model").notNull(),
    
    metadata: jsonb("metadata").$type<Record<string, unknown>>().default({}),
    
    createdAt: timestamp("created_at").notNull().defaultNow(),
  },
  (table) => ({
    userIdx: index("rag_retrieval_logs_user_idx").on(table.userId),
    sessionIdx: index("rag_retrieval_logs_session_idx").on(table.sessionId),
    createdAtIdx: index("rag_retrieval_logs_created_at_idx").on(table.createdAt),
  })
);
```

---

### Step 2: æ–‡æ¡£å¤„ç†å·¥å…· (3 å°æ—¶)

#### 2.1 æ–‡æœ¬åˆ†å—å·¥å…·
**æ–‡ä»¶**: `src/lib/rag/text-chunker.ts`

```typescript
/**
 * æ–‡æœ¬åˆ†å—ç­–ç•¥
 * - æŒ‰æ®µè½åˆ†å— (ä¼˜å…ˆ)
 * - æŒ‰å­—ç¬¦æ•°åˆ†å— (fallback)
 * - ä¿ç•™é‡å éƒ¨åˆ† (overlap)
 */

export interface ChunkOptions {
  maxChunkSize: number; // æœ€å¤§å­—ç¬¦æ•° (é»˜è®¤ 1000)
  overlap: number; // é‡å å­—ç¬¦æ•° (é»˜è®¤ 200)
  separator: string; // åˆ†éš”ç¬¦ (é»˜è®¤ \n\n)
}

export interface TextChunk {
  content: string;
  index: number;
  metadata: {
    startChar: number;
    endChar: number;
    parentLength: number;
  };
}

export class TextChunker {
  private options: ChunkOptions;

  constructor(options?: Partial<ChunkOptions>) {
    this.options = {
      maxChunkSize: options?.maxChunkSize || 1000,
      overlap: options?.overlap || 200,
      separator: options?.separator || "\n\n",
    };
  }

  /**
   * åˆ†å—æ–‡æœ¬
   */
  chunk(text: string): TextChunk[] {
    // 1. æŒ‰æ®µè½åˆ†å‰²
    const paragraphs = text.split(this.options.separator);
    
    const chunks: TextChunk[] = [];
    let currentChunk = "";
    let currentIndex = 0;
    let charPosition = 0;

    for (const paragraph of paragraphs) {
      // å¦‚æœå½“å‰æ®µè½è¿‡é•¿ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ†
      if (paragraph.length > this.options.maxChunkSize) {
        // ä¿å­˜å½“å‰å—
        if (currentChunk) {
          chunks.push({
            content: currentChunk.trim(),
            index: currentIndex++,
            metadata: {
              startChar: charPosition - currentChunk.length,
              endChar: charPosition,
              parentLength: text.length,
            },
          });
          currentChunk = "";
        }

        // æŒ‰å­—ç¬¦åˆ‡åˆ†é•¿æ®µè½
        const subChunks = this.chunkByCharacters(paragraph);
        for (const subChunk of subChunks) {
          chunks.push({
            content: subChunk,
            index: currentIndex++,
            metadata: {
              startChar: charPosition,
              endChar: charPosition + subChunk.length,
              parentLength: text.length,
            },
          });
          charPosition += subChunk.length;
        }
      } else {
        // å¦‚æœåŠ ä¸Šå½“å‰æ®µè½è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œä¿å­˜å½“å‰å—
        if (currentChunk.length + paragraph.length > this.options.maxChunkSize) {
          chunks.push({
            content: currentChunk.trim(),
            index: currentIndex++,
            metadata: {
              startChar: charPosition - currentChunk.length,
              endChar: charPosition,
              parentLength: text.length,
            },
          });

          // ä¿ç•™é‡å éƒ¨åˆ†
          const overlapText = currentChunk.slice(-this.options.overlap);
          currentChunk = overlapText + this.options.separator + paragraph;
        } else {
          currentChunk += (currentChunk ? this.options.separator : "") + paragraph;
        }
        charPosition += paragraph.length + this.options.separator.length;
      }
    }

    // ä¿å­˜æœ€åä¸€ä¸ªå—
    if (currentChunk.trim()) {
      chunks.push({
        content: currentChunk.trim(),
        index: currentIndex,
        metadata: {
          startChar: charPosition - currentChunk.length,
          endChar: charPosition,
          parentLength: text.length,
        },
      });
    }

    return chunks;
  }

  /**
   * æŒ‰å­—ç¬¦æ•°åˆ‡åˆ† (ç”¨äºè¶…é•¿æ®µè½)
   */
  private chunkByCharacters(text: string): string[] {
    const chunks: string[] = [];
    let start = 0;

    while (start < text.length) {
      const end = Math.min(start + this.options.maxChunkSize, text.length);
      chunks.push(text.slice(start, end));
      start = end - this.options.overlap; // é‡å 
    }

    return chunks;
  }
}
```

#### 2.2 å‘é‡åŒ–æœåŠ¡
**æ–‡ä»¶**: `src/lib/rag/embedding-service.ts`

```typescript
import { openai } from "@ai-sdk/openai";
import { embed } from "ai";

/**
 * OpenAI Embeddings æœåŠ¡
 */
export class EmbeddingService {
  private model = openai.embedding("text-embedding-3-small");

  /**
   * ç”Ÿæˆå•ä¸ªæ–‡æœ¬çš„å‘é‡
   */
  async generateEmbedding(text: string): Promise<number[]> {
    try {
      const { embedding } = await embed({
        model: this.model,
        value: text,
      });

      return embedding;
    } catch (error) {
      console.error("Generate embedding error:", error);
      throw new Error("Failed to generate embedding");
    }
  }

  /**
   * æ‰¹é‡ç”Ÿæˆå‘é‡
   */
  async generateEmbeddings(texts: string[]): Promise<number[][]> {
    try {
      const embeddings = await Promise.all(
        texts.map((text) => this.generateEmbedding(text))
      );

      return embeddings;
    } catch (error) {
      console.error("Generate embeddings batch error:", error);
      throw new Error("Failed to generate embeddings");
    }
  }
}

export const embeddingService = new EmbeddingService();
```

---

### Step 3: RAG æ£€ç´¢å¼•æ“ (3 å°æ—¶)

#### 3.1 å‘é‡æ£€ç´¢æœåŠ¡
**æ–‡ä»¶**: `src/lib/rag/vector-search.ts`

```typescript
import { db } from "@/db";
import { knowledgeDocuments } from "@/db/schema-knowledge";
import { sql } from "drizzle-orm";
import { embeddingService } from "./embedding-service";

export interface SearchOptions {
  category?: string; // è¿‡æ»¤åˆ†ç±»
  topK?: number; // è¿”å›å‰ K ä¸ªç»“æœ (é»˜è®¤ 3)
  threshold?: number; // ç›¸ä¼¼åº¦é˜ˆå€¼ (é»˜è®¤ 0.7)
}

export interface SearchResult {
  id: string;
  title: string;
  content: string;
  category: string;
  source: string;
  similarity: number;
  metadata: Record<string, unknown>;
}

export class VectorSearchService {
  /**
   * è¯­ä¹‰æœç´¢
   */
  async search(
    query: string,
    options: SearchOptions = {}
  ): Promise<SearchResult[]> {
    const { category, topK = 3, threshold = 0.7 } = options;

    try {
      // 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
      const queryEmbedding = await embeddingService.generateEmbedding(query);

      // 2. å‘é‡ç›¸ä¼¼åº¦æœç´¢ (ä½™å¼¦ç›¸ä¼¼åº¦)
      const results = await db.execute(sql`
        SELECT 
          id,
          title,
          content,
          category,
          source,
          metadata,
          1 - (embedding <=> ${JSON.stringify(queryEmbedding)}::vector) AS similarity
        FROM knowledge_documents
        WHERE 
          ${category ? sql`category = ${category} AND` : sql``}
          1 - (embedding <=> ${JSON.stringify(queryEmbedding)}::vector) > ${threshold}
        ORDER BY embedding <=> ${JSON.stringify(queryEmbedding)}::vector
        LIMIT ${topK}
      `);

      return results.rows.map((row: any) => ({
        id: row.id,
        title: row.title,
        content: row.content,
        category: row.category,
        source: row.source,
        similarity: parseFloat(row.similarity),
        metadata: row.metadata || {},
      }));
    } catch (error) {
      console.error("Vector search error:", error);
      throw new Error("Failed to perform vector search");
    }
  }

  /**
   * æ··åˆæœç´¢ (å‘é‡ + å…³é”®è¯)
   */
  async hybridSearch(
    query: string,
    options: SearchOptions = {}
  ): Promise<SearchResult[]> {
    // æœªæ¥å¯ä»¥å®ç° BM25 + Vector çš„æ··åˆæ£€ç´¢
    return this.search(query, options);
  }
}

export const vectorSearchService = new VectorSearchService();
```

#### 3.2 RAG ç”ŸæˆæœåŠ¡
**æ–‡ä»¶**: `src/lib/rag/rag-generator.ts`

```typescript
import { openai } from "@ai-sdk/openai";
import { generateText } from "ai";
import { vectorSearchService, type SearchResult } from "./vector-search";
import { db } from "@/db";
import { ragRetrievalLogs } from "@/db/schema-knowledge";
import { embeddingService } from "./embedding-service";

export interface RAGOptions {
  category?: string;
  topK?: number;
  model?: string;
  temperature?: number;
}

export interface RAGResult {
  response: string;
  references: SearchResult[];
  metadata: {
    retrievalCount: number;
    model: string;
    tokensUsed?: number;
  };
}

export class RAGGenerator {
  /**
   * RAG å¢å¼ºç”Ÿæˆ
   */
  async generate(
    query: string,
    userId: string,
    sessionId?: string,
    options: RAGOptions = {}
  ): Promise<RAGResult> {
    const {
      category,
      topK = 3,
      model = "gpt-4o-mini",
      temperature = 0.7,
    } = options;

    try {
      // 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
      const references = await vectorSearchService.search(query, {
        category,
        topK,
      });

      if (references.length === 0) {
        // æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ï¼Œé™çº§åˆ°æ™®é€šç”Ÿæˆ
        return this.generateWithoutRAG(query, model, temperature);
      }

      // 2. æ„å»º Prompt (åŒ…å«æ£€ç´¢åˆ°çš„çŸ¥è¯†)
      const context = references
        .map(
          (ref, index) =>
            `ã€å‚è€ƒèµ„æ–™ ${index + 1}ã€‘\næ¥æº: ${ref.source}\nå†…å®¹: ${ref.content}\nç›¸ä¼¼åº¦: ${(ref.similarity * 100).toFixed(1)}%`
        )
        .join("\n\n");

      const prompt = `ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å…«å­—é£æ°´å¤§å¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

${context}

ç”¨æˆ·é—®é¢˜: ${query}

è¯·æ³¨æ„:
1. ä¼˜å…ˆä½¿ç”¨å‚è€ƒèµ„æ–™ä¸­çš„ä¿¡æ¯
2. å¦‚æœå‚è€ƒèµ„æ–™ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å›ç­”è¦ä¸“ä¸šã€å‡†ç¡®ã€æ˜“æ‡‚
4. å¦‚æœ‰å¼•ç”¨ï¼Œè¯·æ³¨æ˜å‡ºå¤„

ä½ çš„å›ç­”:`;

      // 3. ç”Ÿæˆå›ç­”
      const { text, usage } = await generateText({
        model: openai(model),
        prompt,
        temperature,
      });

      // 4. è®°å½•æ£€ç´¢æ—¥å¿—
      const queryEmbedding = await embeddingService.generateEmbedding(query);
      
      await db.insert(ragRetrievalLogs).values({
        userId,
        sessionId,
        query,
        queryEmbedding: JSON.stringify(queryEmbedding),
        retrievedDocIds: references.map((ref) => ref.id),
        topK,
        similarityScores: references.map((ref) => ref.similarity),
        generatedResponse: text,
        model,
        metadata: {
          tokensUsed: usage?.totalTokens,
        },
      });

      // 5. æ›´æ–°æ–‡æ¡£å¼•ç”¨è®¡æ•°
      for (const ref of references) {
        await db.execute(sql`
          UPDATE knowledge_documents
          SET reference_count = reference_count + 1
          WHERE id = ${ref.id}
        `);
      }

      return {
        response: text,
        references,
        metadata: {
          retrievalCount: references.length,
          model,
          tokensUsed: usage?.totalTokens,
        },
      };
    } catch (error) {
      console.error("RAG generation error:", error);
      throw new Error("Failed to generate RAG response");
    }
  }

  /**
   * é™çº§: æ—  RAG ç”Ÿæˆ
   */
  private async generateWithoutRAG(
    query: string,
    model: string,
    temperature: number
  ): Promise<RAGResult> {
    const { text } = await generateText({
      model: openai(model),
      prompt: `ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å…«å­—é£æ°´å¤§å¸ˆã€‚è¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š\n\n${query}`,
      temperature,
    });

    return {
      response: text,
      references: [],
      metadata: {
        retrievalCount: 0,
        model,
      },
    };
  }
}

export const ragGenerator = new RAGGenerator();
```

---

### Step 4: çŸ¥è¯†åº“ç®¡ç† (2 å°æ—¶)

#### 4.1 æ–‡æ¡£å¯¼å…¥è„šæœ¬
**æ–‡ä»¶**: `scripts/import-knowledge-base.ts`

```typescript
import { db } from "../src/db";
import { knowledgeDocuments } from "../src/db/schema-knowledge";
import { TextChunker } from "../src/lib/rag/text-chunker";
import { embeddingService } from "../src/lib/rag/embedding-service";
import fs from "fs";
import path from "path";

interface DocumentInput {
  title: string;
  category: "bazi" | "fengshui" | "faq" | "case";
  source: string;
  filePath: string;
}

async function importDocument(doc: DocumentInput) {
  console.log(`Importing: ${doc.title}...`);

  // 1. è¯»å–æ–‡ä»¶
  const content = fs.readFileSync(doc.filePath, "utf-8");

  // 2. åˆ†å—
  const chunker = new TextChunker({ maxChunkSize: 1000, overlap: 200 });
  const chunks = chunker.chunk(content);

  console.log(`  - Split into ${chunks.length} chunks`);

  // 3. ç”Ÿæˆå‘é‡
  const chunkContents = chunks.map((chunk) => chunk.content);
  const embeddings = await embeddingService.generateEmbeddings(chunkContents);

  console.log(`  - Generated ${embeddings.length} embeddings`);

  // 4. æ’å…¥æ•°æ®åº“
  for (let i = 0; i < chunks.length; i++) {
    await db.insert(knowledgeDocuments).values({
      title: `${doc.title} (ç¬¬ ${i + 1} éƒ¨åˆ†)`,
      category: doc.category,
      source: doc.source,
      content: chunks[i].content,
      embedding: JSON.stringify(embeddings[i]),
      chunkIndex: i,
      metadata: chunks[i].metadata,
    });
  }

  console.log(`âœ“ Imported: ${doc.title}\n`);
}

async function main() {
  const documents: DocumentInput[] = [
    {
      title: "æ»´å¤©é«“ç²¾å",
      category: "bazi",
      source: "æ»´å¤©é«“",
      filePath: path.join(__dirname, "../knowledge-base/bazi/ditianmiao.txt"),
    },
    {
      title: "ç„ç©ºé£æ˜ŸåŸºç¡€",
      category: "fengshui",
      source: "ç„ç©ºé£æ˜Ÿç§˜ç±",
      filePath: path.join(__dirname, "../knowledge-base/fengshui/xuankong.txt"),
    },
    // æ›´å¤šæ–‡æ¡£...
  ];

  for (const doc of documents) {
    try {
      await importDocument(doc);
    } catch (error) {
      console.error(`Failed to import ${doc.title}:`, error);
    }
  }

  console.log("Import completed!");
  process.exit(0);
}

main();
```

---

### Step 5: å‰ç«¯é›†æˆ (3 å°æ—¶)

#### 5.1 çŸ¥è¯†å¼•ç”¨ç»„ä»¶
**æ–‡ä»¶**: `src/components/rag/knowledge-reference.tsx`

```typescript
"use client";

import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { Badge } from "@/components/ui/badge";
import { BookOpen, ExternalLink } from "lucide-react";
import type { SearchResult } from "@/lib/rag/vector-search";

interface KnowledgeReferenceProps {
  references: SearchResult[];
}

export function KnowledgeReference({ references }: KnowledgeReferenceProps) {
  if (references.length === 0) {
    return null;
  }

  return (
    <Card className="border-blue-200 bg-blue-50/50">
      <CardHeader>
        <CardTitle className="text-sm flex items-center gap-2">
          <BookOpen className="w-4 h-4" />
          å‚è€ƒèµ„æ–™ ({references.length})
        </CardTitle>
      </CardHeader>
      <CardContent className="space-y-3">
        {references.map((ref, index) => (
          <div
            key={ref.id}
            className="p-3 bg-white rounded-lg border border-blue-100"
          >
            <div className="flex items-start justify-between gap-2 mb-2">
              <p className="font-medium text-sm">{ref.title}</p>
              <Badge variant="secondary" className="text-xs">
                {(ref.similarity * 100).toFixed(0)}%
              </Badge>
            </div>
            <p className="text-xs text-muted-foreground line-clamp-2">
              {ref.content}
            </p>
            <div className="flex items-center gap-2 mt-2">
              <span className="text-xs text-muted-foreground">
                æ¥æº: {ref.source}
              </span>
              <ExternalLink className="w-3 h-3 text-muted-foreground" />
            </div>
          </div>
        ))}
      </CardContent>
    </Card>
  );
}
```

---

## ğŸ“Š éªŒæ”¶æ ‡å‡†

| æ ‡å‡† | æ£€æŸ¥é¡¹ |
|------|--------|
| âœ… æ•°æ®åº“ Schema | pgvector æ‰©å±•ã€å‘é‡ç´¢å¼• |
| âœ… æ–‡æœ¬åˆ†å— | 1000 å­—ç¬¦/å—,  200 é‡å  |
| âœ… å‘é‡ç”Ÿæˆ | OpenAI Embeddings 1536 ç»´ |
| âœ… ç›¸ä¼¼åº¦æœç´¢ | ä½™å¼¦ç›¸ä¼¼åº¦ > 0.7 |
| âœ… RAG ç”Ÿæˆ | åŒ…å«å‚è€ƒèµ„æ–™çš„å›ç­” |
| âœ… çŸ¥è¯†å¼•ç”¨å±•ç¤º | å‰ç«¯ç»„ä»¶æ˜¾ç¤ºå¼•ç”¨ |
| âœ… æ£€ç´¢æ—¥å¿— | è®°å½•æŸ¥è¯¢å’Œç»“æœ |
| âœ… æ€§èƒ½ | æ£€ç´¢ < 200ms |

---

## ğŸ’° æˆæœ¬ä¼°ç®—

### OpenAI Embeddings æˆæœ¬
- text-embedding-3-small: $0.00002 / 1K tokens
- 1000 å­—æ–‡æ¡£ â‰ˆ 400 tokens
- 1000 å­—æ–‡æ¡£å‘é‡åŒ–: $0.000008
- 10,000 æ¡æ–‡æ¡£: $0.08

### æŸ¥è¯¢æˆæœ¬
- æ¯æ¬¡æŸ¥è¯¢ç”Ÿæˆå‘é‡: $0.000008
- GPT-4o-mini ç”Ÿæˆ: $0.15 / 1M input tokens
- å•æ¬¡ RAG æŸ¥è¯¢ (å« 3 ä¸ªå‚è€ƒ): ~$0.01

### æ€»æˆæœ¬
- åˆå§‹åŒ–çŸ¥è¯†åº“: $0.08 (ä¸€æ¬¡æ€§)
- å•æ¬¡ RAG æŸ¥è¯¢: $0.01
- **æœˆæˆæœ¬ (1000 æ¬¡æŸ¥è¯¢)**: ~$10

---

## ğŸ”„ ä¸‹ä¸€æ­¥ (Phase 8)

- Pro è®¢é˜…æœˆåº¦è¿åŠ¿
- å®šæ—¶ä»»åŠ¡è°ƒåº¦
- é€šçŸ¥æ¨é€æœºåˆ¶

---

**æ–‡æ¡£ç”Ÿæˆæ—¶é—´**: 2025-01-12 03:00 UTC+8  
**Phase 7 çŠ¶æ€**: â³ è®¡åˆ’å®Œæˆ  
**é¢„è®¡è€—æ—¶**: 12 å°æ—¶
